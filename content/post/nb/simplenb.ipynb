{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: A Simple Naive Bayes Movie Review Classifier\n",
    "subtitle: Classify movie reviews with a generative model\n",
    "summary: Classify movie reviews with a generative model\n",
    "authors:\n",
    "tags: []\n",
    "categories: []\n",
    "date: \"2019-02-05T00:00:00Z\"\n",
    "lastMod: \"2019-09-05T00:00:00Z\"\n",
    "featured: true\n",
    "draft: false\n",
    "\n",
    "# Featured image\n",
    "# To use, add an image named `featured.jpg/png` to your page's folder. \n",
    "image:\n",
    "  caption: featured.png\n",
    "  focal_point: \"\"\n",
    "\n",
    "# Projects (optional).\n",
    "#   Associate this post with one or more of your projects.\n",
    "#   Simply enter your project's folder or file name without extension.\n",
    "#   E.g. `projects = [\"internal-project\"]` references \n",
    "#   `content/project/deep-learning/index.md`.\n",
    "#   Otherwise, set `projects = []`.\n",
    "projects: []\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews Sentiment Analysis \n",
    "\n",
    "This notebook is a step by step walk-through on how to train a simple naive bayes classfier to recognize wether it the film has received a **positive** or **negative** sentiment\n",
    "\n",
    "Before we start, below is a simple naive bayes equation we will mainly be using for calculating the probabilities.\n",
    "1. Prior: number of files in given class, i.e. if 2 out of 5 reviews are positive, 0.4 will be its prior prob.\n",
    "2. Likelihood or P(feature|class): give num of features, what's the likelihood that its a good film ((i.e. good, amazing, awesome)|positive)\n",
    "3. Evidence: number of data points (here namely reviews) we have. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$$P(A \\mid B) = \\frac{ P( B | A )P( A ) }{P(B)} = {{\\sum}}{ P( B | A )P( A ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this exercise about computing the denominator for the naive Bayes classifier, we can ignore the denominator since we're comparing P(positive|reviews) and P(negative|reviews) and so can cancel out their denominators to simplify our work.\n",
    "\n",
    "I have also attached a picture of the pseudocode demonstration from Juravsky& Martin's SLP textbook.\n",
    "\n",
    "![png](./nb-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Storing Feature Vectors\n",
    "\n",
    "Create parameters to store the **features** into an appropriate data structure of your choice. \n",
    "\n",
    "Here `numpy` is used to create matrices for creating **feature vectors**\n",
    "In the past, I have primarily used `dictionaries` for storing data. Alternatively, `numpy` supports various magic operations on the data structure and is very powerful. Therefore, here `numpy` is used.\n",
    "\n",
    "Please click for more information about how to use [numpy](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {0: 'pos', 1: 'neg'}\n",
    "        self.feature_dict = defaultdict(Counter)\n",
    "        self.prior = np.zeros(2)\n",
    "        self.likelihood = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything, we first have to load data into our workspace.\n",
    "The following code scrapes all the roots, dirs, and directories for the files we need and does some basic\n",
    "text tokenization and counting.\n",
    "\n",
    "We have two classes `doc_class_1/2` preprocessing or `dict_class_1/2` postprocessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def preprocessing(self,train_set):\n",
    "        N_doc = 0 #number of documents\n",
    "        N_class = np.zeros(2) #num of docs in given class\n",
    "        big_doc = defaultdict(Counter)\n",
    "        \n",
    "        for root, dirs, files in os.walk(train_set):\n",
    "            for name in files:\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    tokens = nltk.word_tokenize(f.read())\n",
    "                    if root == r\"Users/shiyishen/Downloads/movie_reviewsâ©/train/action\":\n",
    "                        N_class[0] += 1     # 0 represents action class\n",
    "                        self.feature_dict[0].update(Counter(tokens))\n",
    "                    else:\n",
    "                        N_class[1] += 1     # 1 represents comedy class \n",
    "                        self.feature_dict[1].update(Counter(tokens))\n",
    "                    N_doc += 1\n",
    "                    doc_all.extend(tokens)\n",
    "         \n",
    "        \n",
    "        for i in range(2):\n",
    "            self.prior[i] = np.log()\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our NB Classifier \n",
    "Remember we have defined three parameters `dict_action`, `dict_comedy`, and `doc_all` in the previous cell. As we have iterated through the data folder and loaded in their corresponding text, what's left is count the number of words that each category and the overall data contain. We'll use NLTK's `.FreqDist` to directly compute their **frequency distribution**.\n",
    "\n",
    "Now we can then start to create our **feature vectors**. We do it first by creating an array to store all of our features, which are unique words in our training files. Or you could do your own feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are done with data preprocessing. Let's head into training our model.\n",
    "\n",
    "If you still remember the equation. To calculate the NB distribution of a given class, we need its **prior probability** and the **likelikhood** that each feature appears in the class. \n",
    "Here we use log space to smoothe our calculation, as we might encounter some significantly small number.\n",
    "\n",
    "We'll also be using **Laplace's smoothing** technique also called **add-one smoothing**, in which each word appears one extra time. This is to smooth out words with zero probability that might mess up with our likelihood calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE estimate: \n",
    "$$ P_{MLE}(w_{i} | w_{i-1}) = \\frac{c(w_{i-1},w_{i})}{c(w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add-1(Laplace) estimate:\n",
    "$$ P_{Add-1}(w_{i} | w_{i-1}) = \\frac{c(w_{i-1},w_{i}) + 1}{c(w_{i-1}) + V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing calculating prior, we can go on calculate the likelihood for all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, train_set):\n",
    "        for i in range(len(self.class_dict)):\n",
    "                count_wc = np.zeros(len(self.feature_dict))\n",
    "                sum_count = len(self.feature_dict)   \n",
    "                if i == 0: \n",
    "                    for j in range(len(self.feature_dict)):\n",
    "                        if self.feature_dict[j] in dict_class_1.keys():\n",
    "                            count_wc[j] = dict_class_1[self.feature_dict[j]]\n",
    "                            sum_count += count_wc[j]\n",
    "                        else:\n",
    "                            count_wc[j] = 0\n",
    "                    for j in range(len(self.feature_dict)):\n",
    "                        self.likelihood[0][j] = (count_wc[j] + 1) / sum_count\n",
    "                if i == 1:  \n",
    "                    for j in range(len(self.feature_dict)):\n",
    "\n",
    "                        if self.feature_dict[j] in dict_class_2.keys():\n",
    "                            count_wc[j] = dict_class_2[self.feature_dict[j]]\n",
    "                            sum_count += count_wc[j]\n",
    "                        else:\n",
    "                            count_wc[j] = 0\n",
    "                    for j in range(len(self.feature_dict)):\n",
    "                        self.likelihood[1][j] = (count_wc[j] + 1) / sum_count\n",
    "\n",
    "            self.loglikelihood= np.log(self.likelihood)\n",
    "            return self.loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Classify\n",
    "It's time to predict and classify development set with our newly trained model.\n",
    "\n",
    "One useful method in numpy is the `.dot` method, which gives the product of **a** and **b**.  \n",
    "\n",
    "In such case, the higher the likelihood the more confident that it's the given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, dev_set):\n",
    "    results = defaultdict(dict)\n",
    "    for root, dirs, files in os.walk(dev_set):\n",
    "        for name in files:\n",
    "            if name!='.DS_Store':\n",
    "                feature_vector = np.zeros(len(self.feature_dict))\n",
    "                results[name] = []\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    text = nltk.word_tokenize(f.read())\n",
    "                    if root == '/Users/shiyishen/Downloads/movie_reviews/dev/pos':\n",
    "                        results[name].append('pos')\n",
    "                    else:\n",
    "                        results[name].append('neg')\n",
    "\n",
    "                    dict_text = nltk.FreqDist(text)\n",
    "                    for i in range(len(self.feature_dict)):\n",
    "                        if [i] in text:\n",
    "                            feature_vector[i] = dict_text[self.feature_dict[i]]\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                feature_vector.transpose()\n",
    "                compare = np.dot(self.likelihood, feature_vector)\n",
    "                compare = compare + self.logprior\n",
    "                if compare[0] > compare[1]:\n",
    "                    results[name].append('pos')\n",
    "                elif compare[0] < compare[1]:\n",
    "                    results[name].append('neg')\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \n",
    "\n",
    "Congradulations, if you have reached here! Now, you have successfully created your very own Multinomial Naive Bayes Classifier! To top it off with something more interesting, we can virtually do anything for feature selection.\n",
    "\n",
    "A small tip: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
