{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: A Simple Naive Bayes Movie Review Classifier\n",
    "subtitle: Classify movie reviews with a generative model\n",
    "summary: Classify movie reviews with a generative model\n",
    "authors:\n",
    "tags: []\n",
    "categories: []\n",
    "date: \"2019-02-05T00:00:00Z\"\n",
    "lastMod: \"2019-09-05T00:00:00Z\"\n",
    "featured: true\n",
    "draft: false\n",
    "\n",
    "# Featured image\n",
    "# To use, add an image named `featured.jpg/png` to your page's folder. \n",
    "image:\n",
    "  caption: featured.png\n",
    "  focal_point: \"\"\n",
    "\n",
    "# Projects (optional).\n",
    "#   Associate this post with one or more of your projects.\n",
    "#   Simply enter your project's folder or file name without extension.\n",
    "#   E.g. `projects = [\"internal-project\"]` references \n",
    "#   `content/project/deep-learning/index.md`.\n",
    "#   Otherwise, set `projects = []`.\n",
    "projects: []\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is It An Action or A Comedy Film?\n",
    "\n",
    "This notebook is a step by step walk-through on how to train a simple naive bayes classfier to recognize the genre of the film through its reivew. \n",
    "\n",
    "Before we start, below is a picture demonstration of the equation for calculating the likelihood.\n",
    "1. Prior: number of files in given class, i.e. if 2 out of 5 reviews are action films, 0.4 will be its prior prob.\n",
    "2. Likelihood or P(feature|class): give num of features, what's the likelihood that its an action film ((i.e. fly,fun,kick,hit)|action)\n",
    "3. Evidence: number of data points (here namely reviews) we have. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$$P(A \\mid B) = \\frac{ P( B | A )P( A ) }{P(B)} = {{\\sum}}{ P( B | A )P( A ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this exercise about computing the denominator for the naive Bayes classifier, we can ignore the denominator since we're comparing P(action | review) and P(comedy | review) and so can cancel out their denominators to simplify our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Storing Feature Vectors\n",
    "\n",
    "Create parameters to store the **features** into an appropriate data structure of your choice. \n",
    "\n",
    "Here `numpy` is used to create matrices for creating **feature vectors**\n",
    "In the past, I have primarily used `dictionaries` for storing data. Alternatively, `numpy` supports various magic operations on the data structure and is very powerful. Therefore, here `numpy` is used.\n",
    "\n",
    "Please click for more information about how to use [numpy](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {0: 'action', 1: 'comedy'}\n",
    "        self.feature_dict = {}\n",
    "        self.prior = np.zeros(2)\n",
    "        self.likelihood = None\n",
    "        self.loglikelihood = None\n",
    "        self.logprior = np.zeros(2)\n",
    "        \n",
    "        # be sure to store all the needed data \n",
    "        self.dict_action = None     \n",
    "        self.dict_comedy = None   \n",
    "        self.dict_voc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything, we first have to load data into our workspace.\n",
    "The following code scrapes all the roots, dirs, and directories for the files we need and does some basic\n",
    "text tokenization and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def preprocessing(self,train_set):\n",
    "        prior = np.zeros(2)       #self.prior\n",
    "        N_doc = 0 #number of documents\n",
    "        N_class = np.zeros(2) #\n",
    "        doc_action = []\n",
    "        doc_comedy = []\n",
    "        doc_all = []\n",
    "        for root, dirs, files in os.walk(train_set):\n",
    "            for name in files:\n",
    "                N_doc += 1 #num of documents \n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    text = nltk.word_tokenize(f.read())\n",
    "                if root == r\"Users/shiyishen/doc/class_material/COSI_114_FoCL/homework/PA2/movie_reviews_small‚Å©/train/action\":\n",
    "                    N_class[0] += 1\n",
    "                    doc_action.extend(text)\n",
    "                else:\n",
    "                    N_class[1] += 1\n",
    "                    doc_comedy.extend(text)\n",
    "                doc_all.extend(text)\n",
    "        self.dict_action = nltk.FreqDist(doc_action)      #bigdoc[action]\n",
    "        self.dict_comedy = nltk.FreqDist(doc_comedy)      #bigdoc[comedy]\n",
    "        self.dict_voc = nltk.FreqDist(doc_all)            #vocabulary\n",
    "        \n",
    "        for i in range(2):\n",
    "            self.prior[i] = N_class[i]/N_doc\n",
    "        self.log_prior = np.log(prior)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our NB Classifier \n",
    "Remember we have defined three parameters `dict_action`, `dict_comedy`, and `doc_all` in the previous cell. As we have iterated through the data folder and loaded in their corresponding text, what's left is count the number of words that each category and the overall data contain. We'll use NLTK's `.FreqDist` to directly compute their **frequency distribution**.\n",
    "\n",
    "Now we can then start to create our **feature vectors**. We do it first by creating an array to store all of our features, which are unique words in our training files. Or you could do your own feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are done with data preprocessing. Let's head into training our model.\n",
    "\n",
    "If you still remember the equation. To calculate the NB distribution of a given class, we need its **prior probability** and the **likelikhood** that each feature appears in the class. \n",
    "Here we use log space to smoothe our calculation, as we might encounter some significantly small number.\n",
    "\n",
    "We'll also be using **Laplace's smoothing** technique also called **add-one smoothing**, in which each word appears one extra time. This is to smooth out words with zero probability that might mess up with our likelihood calculation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE estimate: \n",
    "$$ P_{MLE}(w_{i} | w_{i-1}) = \\frac{c(w_{i-1},w_{i})}{c(w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add-1(Laplace) estimate:\n",
    "$$ P_{Add-1}(w_{i} | w_{i-1}) = \\frac{c(w_{i-1},w_{i}) + 1}{c(w_{i-1}) + V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing calculating prior, we can go on calculate the likelihood for all the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self):\n",
    "        for i in range(len(self.class_dict)):\n",
    "                count_wc = np.zeros(len(self.feature_dict))\n",
    "                sum_count = len(self.feature_dict)   \n",
    "                if i == 0:  \n",
    "                    for j in range(len(self.feature_dict)):\n",
    "\n",
    "                        if self.feature_dict[j] in dict_class_1.keys():\n",
    "                            count_wc[j] = dict_class_1[self.feature_dict[j]]\n",
    "                            sum_count += count_wc[j]\n",
    "                        else:\n",
    "                            count_wc[j] = 0\n",
    "                    for j in range(len(self.feature_dict)):\n",
    "                        self.likelihood[0][j] = (count_wc[j] + 1) / sum_count\n",
    "                if i == 1:  \n",
    "                    for j in range(len(self.feature_dict)):\n",
    "\n",
    "                        if self.feature_dict[j] in dict_class_2.keys():\n",
    "                            count_wc[j] = dict_class_2[self.feature_dict[j]]\n",
    "                            sum_count += count_wc[j]\n",
    "                        else:\n",
    "                            count_wc[j] = 0\n",
    "                    for j in range(len(self.feature_dict)):\n",
    "                        self.likelihood[1][j] = (count_wc[j] + 1) / sum_count\n",
    "\n",
    "            self.loglikelihood= np.log(self.likelihood)\n",
    "            return self.loglikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Classify\n",
    "It's time to predict and classify the given file with our newly trained model.\n",
    "\n",
    "We will use `numpy`'s `.dot` method, which does **dot** product matrix operation on the **feature vector** we have just built and compares the parameters. \n",
    "\n",
    "In such case, the higher the likelihood the more confident that it's the given class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(self, dev_set):\n",
    "    results = defaultdict(dict)\n",
    "    for root, dirs, files in os.walk(dev_set):\n",
    "        for name in files:\n",
    "            if name!='.DS_Store':\n",
    "                feature_vector = np.zeros(len(self.feature_dict))\n",
    "                results[name] = []\n",
    "                with open(os.path.join(root, name)) as f:\n",
    "                    text = nltk.word_tokenize(f.read())\n",
    "                    if root == '/Users/shiyishen/Downloads/movie_reviews/dev/pos':\n",
    "                        results[name].append('pos')\n",
    "                    else:\n",
    "                        results[name].append('neg')\n",
    "\n",
    "                    dict_text = nltk.FreqDist(text)\n",
    "                    for i in range(len(self.feature_dict)):\n",
    "                        if [i] in text:\n",
    "                            feature_vector[i] = dict_text[self.feature_dict[i]]\n",
    "                        else:\n",
    "                            pass\n",
    "\n",
    "                feature_vector.transpose()\n",
    "                compare = np.dot(self.loglikelihood, feature_vector)\n",
    "                compare = compare + self.logprior\n",
    "                if compare[0] > compare[1]:\n",
    "                    results[name].append('pos')\n",
    "                elif compare[0] < compare[1]:\n",
    "                    results[name].append('neg')\n",
    "                else:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
