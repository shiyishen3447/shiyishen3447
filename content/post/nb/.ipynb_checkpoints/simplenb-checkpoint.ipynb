{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: A Simple Naive Bayes Movie Review Classifier\n",
    "subtitle: Classify movie reviews with a generative model\n",
    "summary: Classify movie reviews with a generative model\n",
    "authors:\n",
    "tags: []\n",
    "categories: []\n",
    "date: \"2019-02-05T00:00:00Z\"\n",
    "lastMod: \"2019-09-05T00:00:00Z\"\n",
    "featured: true\n",
    "draft: false\n",
    "\n",
    "# Featured image\n",
    "# To use, add an image named `featured.jpg/png` to your page's folder. \n",
    "image:\n",
    "  caption: featured.png\n",
    "  focal_point: \"\"\n",
    "\n",
    "# Projects (optional).\n",
    "#   Associate this post with one or more of your projects.\n",
    "#   Simply enter your project's folder or file name without extension.\n",
    "#   E.g. `projects = [\"internal-project\"]` references \n",
    "#   `content/project/deep-learning/index.md`.\n",
    "#   Otherwise, set `projects = []`.\n",
    "projects: []\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is It An Action or A Comedy Film?\n",
    "\n",
    "This notebook is a step by step walk-through on how to train a simple naive bayes classfier to recognize the genre of the film through its reivew. \n",
    "\n",
    "Before we start, below is a picture demonstration of the equation for calculating the likelihood.\n",
    "1. Prior: number of files in given class, i.e. if 2 out of 5 reviews are action films, 0.4 will be its prior prob.\n",
    "2. Likelihood or P(feature|class): give num of features, what's the likelihood that its an action film ((i.e. fly,fun,kick,hit)|action)\n",
    "3. Evidence: number of data points (here namely reviews) we have. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$$P(A \\mid B) = \\frac{ P( B | A )P( A ) }{P(B)} = {{\\sum}}{ P( B | A )P( A ) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this exercise about computing the denominator for the naive Bayes classifier, we can ignore the denominator since we're comparing P(action | review) and P(comedy | review) and so can cancel out their denominators to simplify our work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Storing Feature Vectors\n",
    "\n",
    "Create parameters to store the **features** into an appropriate data structure of your choice. \n",
    "\n",
    "Here `numpy` is used to create matrices for creating **feature vectors**\n",
    "In the past, I have primarily used `dictionaries` for storing data. Alternatively, `numpy` supports various magic operations on the data structure and is very powerful. Therefore, here `numpy` is used.\n",
    "\n",
    "Please click for more information about how to use [numpy](https://cs231n.github.io/python-numpy-tutorial/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "# nltk.download()\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "prior = np.zeros(2)       #self.prior\n",
    "N_doc = 0 #number of documents\n",
    "N_class = np.zeros(2) #\n",
    "doc_action = []\n",
    "doc_comedy = []\n",
    "doc_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do anything, we first have to load data into our workspace.\n",
    "The following code scrapes all the roots, dirs, and directories for the files we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"Users/shiyishen/doc/class_material/COSI_114_FoCL/homework/PA2/⁨movie_reviews_small/train\"):\n",
    "    for name in files:\n",
    "        N_doc += 1 #num of documents \n",
    "        with open(os.path.join(root, name)) as f:\n",
    "            text = nltk.word_tokenize(f.read())\n",
    "        if root == r\"Users/shiyishen/doc/class_material/COSI_114_FoCL/homework/PA2/movie_reviews_small⁩/train/action\":\n",
    "            N_class[0] += 1\n",
    "            doc_action.extend(text)\n",
    "        else:\n",
    "            N_class[1] += 1\n",
    "            doc_comedy.extend(text)\n",
    "        doc_all.extend(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic text tokenization and counting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_action = nltk.FreqDist(doc_action)      #bigdoc[action]\n",
    "dict_comedy = nltk.FreqDist(doc_comedy)      #bigdoc[comedy]\n",
    "dict_voc = nltk.FreqDist(doc_all)            #vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we have defined three parameters `dict_action`, `dict_comedy`, and `doc_all` in the previous cell. As we have iterated through the data folder and loaded in their corresponding text, what's left is count the number of words that each category and the overall data contain. We'll use NLTK's `.FreqDist` to directly compute their **frequency distribution**.\n",
    "\n",
    "## Training Our NB Classifier \n",
    "Now we can then start to create our **feature vectors**. We do it first by creating an array to store all of our features, which are unique words in our training files. Or you could do your own feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_features = len(dict_voc.keys())\n",
    "likelihood = np.zeros((2, N_features)) #for storing all the likelihood for each feature\n",
    "class_dict = ['action', 'comedy']\n",
    "features = []\n",
    "for i in dict_voc.keys():\n",
    "    feature_dict.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are done with data prep and preprocessing. Let's head into training our model \n",
    "\n",
    "If you still remember the equation. To calculate the NB distribution of a given class, we need its **prior probability** and the **likelikhood** that each feature appears in the class. \n",
    "Here we use log pace to smoothe our calculation, as we might encounter some significantly small number.\n",
    "\n",
    "We use **Laplace's smoothing** technique also called **add-one smoothing**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prior = np.zeros(len(prior))\n",
    "log_likelihood = None \n",
    "\n",
    "for i in range(2):\n",
    "    log_prior[i] = np.log(N_class[i]/N_doc)\n",
    "\n",
    "for i in range(len(class_dict)):\n",
    "    count_wc = np.zeros(len(features))\n",
    "    sum_count = len(features)  \n",
    "    if i == 0:  \n",
    "        for j in range(len(features)):\n",
    "\n",
    "            if features[j] in dict_action.keys():\n",
    "                count_wc[j] = dict_action[features[j]]\n",
    "                sum_count += count_wc[j] \n",
    "            else:\n",
    "                count_wc[j] = 0\n",
    "        for j in range(len(features)):\n",
    "            likelihood[0][j] = (count_wc[j] + 1) / sum_count\n",
    "    if i == 1: \n",
    "        for j in range(len(features)):\n",
    "\n",
    "            if features[j] in dict_comedy.keys():\n",
    "                count_wc[j] = dict_comedy[features[j]]\n",
    "                sum_count += count_wc[j]\n",
    "            else:\n",
    "                count_wc[j] = 0\n",
    "        for j in range(len(features)):\n",
    "            likelihood[1][j] = (count_wc[j] + 1) / sum_count\n",
    "log_likelihood = np.log(likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Classify\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = defaultdict(dict)\n",
    "for root, dirs, files in os.walk(dev_set):\n",
    "    for name in files:\n",
    "        #print(name)\n",
    "        if name!='.DS_Store':\n",
    "            feature_vector = np.zeros(len(self.feature_dict))\n",
    "            results[name] = []\n",
    "            with open(os.path.join(root, name)) as f:\n",
    "                text = nltk.word_tokenize(f.read())\n",
    "                #print(text)\n",
    "                if root == '/Users/shiyishen/Downloads/movie_reviews/dev/pos':\n",
    "                    results[name].append('pos')\n",
    "                else:\n",
    "                    results[name].append('neg')\n",
    "\n",
    "                dict_text = nltk.FreqDist(text)\n",
    "                for i in range(len(self.feature_dict)):\n",
    "                    if [i] in text:\n",
    "                        feature_vector[i] = dict_text[self.feature_dict[i]]\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            feature_vector.transpose()\n",
    "            compare = np.dot(self.loglikelihood, feature_vector)\n",
    "            compare = compare + self.logprior\n",
    "            if compare[0] > compare[1]:\n",
    "                results[name].append('pos')\n",
    "            else:\n",
    "                results[name].append('neg')\n",
    "            # create feature vectors for each document\n",
    "                pass\n",
    "        # get most likely class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
